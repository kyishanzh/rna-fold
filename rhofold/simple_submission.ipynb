{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and prepare data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.\n",
      "/srv/mingyang/dev/personal/rna-fold/rhofold/rhofold/utils/rigid_utils.py:35: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403590347/work/aten/src/ATen/native/Cross.cpp:63.)\n",
      "  e3 = torch.cross(e1, e2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RhoFold(\n",
       "  (msa_embedder): MSAEmbedder(\n",
       "    (msa_emb): MSANet(\n",
       "      (embed_tokens): Embedding(17, 256, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(4098, 256, padding_idx=1)\n",
       "    )\n",
       "    (pair_emb): PairNet(\n",
       "      (pair_emb): PairEmbNet(\n",
       "        (emb): Embedding(17, 64)\n",
       "        (projection): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (pos): PositionalEncoding2D(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rna_fm): ProteinBertModel(\n",
       "      (embed_tokens): Embedding(25, 640, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "          (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (contact_head): ContactPredictionHead(\n",
       "        (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 640, padding_idx=1)\n",
       "      (emb_layer_norm_before): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rna_fm_reduction): Linear(in_features=896, out_features=256, bias=True)\n",
       "  )\n",
       "  (e2eformer): E2EformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x E2EformerBlock(\n",
       "        (msa_att_row): MSARowAttentionWithPairBias(\n",
       "          (layer_norm_m): LayerNorm()\n",
       "          (layer_norm_z): LayerNorm()\n",
       "          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (linear_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (linear_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (linear_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (linear_g): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (msa_att_col): MSAColumnAttention(\n",
       "          (_msa_att): MSAAttention(\n",
       "            (layer_norm_m): LayerNorm()\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (linear_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (linear_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (linear_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (linear_g): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (core): E2EformerBlockCore(\n",
       "          (msa_transition): MSATransition(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (relu): ReLU()\n",
       "            (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (outer_product_mean): OuterProductMean(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear_1): Linear(in_features=256, out_features=32, bias=True)\n",
       "            (linear_2): Linear(in_features=256, out_features=32, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (pair_transition): PairTransition(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (relu): ReLU()\n",
       "            (linear_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=384, bias=True)\n",
       "  )\n",
       "  (structure_module): StructureModule(\n",
       "    (layer_norm_s): LayerNorm()\n",
       "    (layer_norm_z): LayerNorm()\n",
       "    (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (ipa): InvariantPointAttention(\n",
       "      (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "      (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "      (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "      (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (softplus): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "    (layer_norm_ipa): LayerNorm()\n",
       "    (transition): StructureModuleTransition(\n",
       "      (layers): ModuleList(\n",
       "        (0): StructureModuleTransitionLayer(\n",
       "          (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm()\n",
       "    )\n",
       "    (bb_update): BackboneUpdate(\n",
       "      (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "    )\n",
       "    (angle_resnet): AngleResnet(\n",
       "      (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x AngleResnetBlock(\n",
       "          (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (linear_out): Linear(in_features=128, out_features=12, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (refinenet): RefineNet(\n",
       "      (embed_tokens): Embedding(17, 64)\n",
       "      (embed_positions): PosEmbedding(4098, 64, padding_idx=1)\n",
       "      (refine_layer0): ResEGNN(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x EGNN(\n",
       "            (edge_mlp): Sequential(\n",
       "              (0): Linear(in_features=129, out_features=258, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=258, out_features=32, bias=True)\n",
       "              (3): SiLU()\n",
       "            )\n",
       "            (coors_norm): CoorsNorm(\n",
       "              (fn): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (node_mlp): Sequential(\n",
       "              (0): Linear(in_features=96, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "            (coors_mlp): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (refine_layer1): ResEGNN(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x EGNN(\n",
       "            (edge_mlp): Sequential(\n",
       "              (0): Linear(in_features=129, out_features=258, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=258, out_features=32, bias=True)\n",
       "              (3): SiLU()\n",
       "            )\n",
       "            (coors_norm): CoorsNorm(\n",
       "              (fn): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (node_mlp): Sequential(\n",
       "              (0): Linear(in_features=96, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "            (coors_mlp): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (refine_layer2): ResEGNN(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x EGNN(\n",
       "            (edge_mlp): Sequential(\n",
       "              (0): Linear(in_features=129, out_features=258, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=258, out_features=32, bias=True)\n",
       "              (3): SiLU()\n",
       "            )\n",
       "            (coors_norm): CoorsNorm(\n",
       "              (fn): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (node_mlp): Sequential(\n",
       "              (0): Linear(in_features=96, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "            (coors_mlp): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (refine_layer3): ResEGNN(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x EGNN(\n",
       "            (edge_mlp): Sequential(\n",
       "              (0): Linear(in_features=129, out_features=258, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=258, out_features=32, bias=True)\n",
       "              (3): SiLU()\n",
       "            )\n",
       "            (coors_norm): CoorsNorm(\n",
       "              (fn): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (node_mlp): Sequential(\n",
       "              (0): Linear(in_features=96, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "            (coors_mlp): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (recycle_embnet): RecyclingEmbedder(\n",
       "    (linear): Linear(in_features=40, out_features=128, bias=True)\n",
       "    (layer_norm_m): LayerNorm()\n",
       "    (layer_norm_z): LayerNorm()\n",
       "  )\n",
       "  (dist_head): DistHead(\n",
       "    (norm): LayerNorm()\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (resnet_dist_0): FeedForwardLayer(\n",
       "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (post_act_ln): Identity()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (resnet_dist_1): FeedForwardLayer(\n",
       "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (post_act_ln): Identity()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (resnet_dist_2): FeedForwardLayer(\n",
       "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (post_act_ln): Identity()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=512, out_features=40, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (ss_head): SSHead(\n",
       "    (norm): LayerNorm()\n",
       "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (ffn): FeedForwardLayer(\n",
       "      (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (post_act_ln): Identity()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (plddt_head): pLDDTHead(\n",
       "    (net_lddt): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=384, out_features=50, bias=True)\n",
       "    )\n",
       "    (sfmx): Softmax(dim=2)\n",
       "  )\n",
       "  (evo2_head): Linear(in_features=8192, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from rhofold.data.balstn import BLASTN\n",
    "from rhofold.rhofold import RhoFold\n",
    "from rhofold.config import rhofold_config\n",
    "from rhofold.utils import get_device, save_ss2ct, timing\n",
    "from rhofold.relax.relax import AmberRelaxation\n",
    "from rhofold.utils.alphabet import get_features\n",
    "\n",
    "\n",
    "def load_model(ckpt=\"./pretrained/RhoFold_pretrained.pt\"):\n",
    "    model = RhoFold(rhofold_config)\n",
    "    model.load_state_dict(\n",
    "        torch.load(ckpt, map_location=torch.device(\"cpu\"))[\"model\"], strict=False\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "device = \"cuda\"\n",
    "model = load_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1SCL_A\n",
      "1RNK_A\n",
      "1RHT_A\n",
      "1HLX_A\n",
      "1HMH_E\n",
      "1RNG_A\n",
      "1MME_D\n",
      "1KAJ_A\n",
      "1SLO_A\n",
      "1BIV_A\n",
      "1ANR_A\n",
      "1ZIG_A\n",
      "1ZIH_A\n",
      "1ETF_A\n",
      "1ZIF_A\n",
      "1KPD_A\n",
      "1IKD_A\n",
      "1ZDI_S\n",
      "1AFX_A\n",
      "1EBQ_A\n",
      "1EBR_A\n",
      "1ULL_A\n",
      "1KIS_B\n",
      "1KIS_A\n",
      "1ATO_A\n",
      "1TLR_A\n",
      "1VOP_A\n",
      "1AQO_A\n",
      "1ATV_A\n",
      "1ATW_A\n",
      "1UUU_A\n",
      "1AUD_B\n",
      "2U2A_A\n",
      "1A4T_A\n",
      "1A60_A\n",
      "1A51_A\n",
      "2A9L_A\n",
      "1A1T_B\n",
      "1A9N_Q\n",
      "3PHP_A\n",
      "2TPK_A\n",
      "7MSF_S\n",
      "5MSF_S\n",
      "1LDZ_A\n",
      "1ZDK_S\n",
      "1BVJ_A\n",
      "1B36_A\n",
      "1HVU_I\n",
      "1BGZ_A\n",
      "2BJ2_B\n",
      "2BJ2_A\n",
      "28SP_A\n",
      "1QFQ_A\n",
      "17RA_A\n",
      "1BAU_B\n",
      "1BZ2_A\n",
      "1CQ5_A\n",
      "1QC8_A\n",
      "484D_B\n",
      "1D6K_B\n",
      "1EIY_C\n",
      "1ESH_A\n",
      "1EXY_A\n",
      "1D0U_A\n",
      "1ESY_A\n",
      "1EUQ_B\n",
      "1F9L_A\n",
      "1FFK_9\n",
      "1EKZ_B\n",
      "1F5U_B\n",
      "1F6Z_A\n",
      "1F6X_A\n",
      "1F85_A\n",
      "1F84_A\n",
      "1FOQ_A\n",
      "1FJE_A\n",
      "1E4P_A\n",
      "1FQZ_A\n",
      "1G70_A\n",
      "1E7K_D\n",
      "1FYO_A\n",
      "1FHK_A\n",
      "1I3X_A\n",
      "1I4C_A\n",
      "1I46_A\n",
      "1IBM_Y\n",
      "1HWQ_A\n",
      "1IK1_A\n",
      "1E95_A\n",
      "1HS2_A\n",
      "1JO7_A\n",
      "1IDV_A\n",
      "1K5I_A\n",
      "1K9W_A\n",
      "1JTW_A\n",
      "1K6G_A\n",
      "1K4B_A\n",
      "1K4A_A\n",
      "1K6H_A\n",
      "1JUR_A\n",
      "1KKS_A\n",
      "1JWC_A\n",
      "1L1C_C\n",
      "1KP7_A\n",
      "1JOX_A\n",
      "1K2G_A\n",
      "1L1W_A\n",
      "1LC6_A\n",
      "1LS2_B\n",
      "1KKA_A\n",
      "1MFY_A\n",
      "1M5L_A\n",
      "1MFJ_A\n",
      "1MFK_A\n",
      "1N34_A\n",
      "1JTJ_A\n",
      "1MT4_A\n",
      "1MNX_A\n",
      "1NA2_A\n",
      "1NC0_A\n",
      "1N8X_A\n",
      "1OQ0_A\n",
      "1OW9_A\n",
      "1OSW_A\n",
      "1M82_A\n",
      "1NYB_B\n",
      "1PJY_A\n",
      "1P6V_B\n",
      "1P6V_D\n",
      "1N66_A\n",
      "1HS1_A\n",
      "1HS8_A\n",
      "1HS4_A\n",
      "1HS3_A\n",
      "1JZC_A\n",
      "1QZC_C\n",
      "1P5M_A\n",
      "1R2W_C\n",
      "1P5N_A\n",
      "1QZC_B\n",
      "1P5P_A\n",
      "1QZA_B\n",
      "1QZB_B\n",
      "1QWB_A\n",
      "1QWA_A\n",
      "1Q75_A\n",
      "1R2P_A\n",
      "1RFR_A\n",
      "1S9S_A\n",
      "1RY1_E\n",
      "1SZY_A\n",
      "1ROQ_A\n",
      "1R7W_A\n",
      "1R7Z_A\n",
      "1T4L_A\n",
      "1WKS_A\n",
      "1T28_A\n",
      "1S34_A\n",
      "1TJZ_A\n",
      "1TXS_A\n",
      "1R4H_A\n",
      "1XHP_A\n",
      "1XSG_A\n",
      "1Y1Y_P\n",
      "1XSH_A\n",
      "1YLG_A\n",
      "1YNE_A\n",
      "1XWU_A\n",
      "1XWP_A\n",
      "1TBK_A\n",
      "1YMO_A\n",
      "1Z31_A\n",
      "1U63_D\n",
      "1ZC8_Z\n",
      "1ZC8_G\n",
      "1ZC8_J\n",
      "1ZC8_F\n",
      "1ZC8_I\n",
      "1ZC8_H\n",
      "1Z30_A\n",
      "1X18_D\n",
      "1X18_A\n",
      "1X18_B\n",
      "1ZC5_A\n",
      "1ZN1_B\n",
      "1Z2J_A\n",
      "1YSH_B\n",
      "1YSH_A\n",
      "1YSH_F\n",
      "1YSV_A\n",
      "2ADT_A\n",
      "1WZ2_D\n",
      "2A64_A\n",
      "2D19_A\n",
      "2D1B_B\n",
      "2EUY_A\n",
      "2B6G_B\n",
      "2ESE_B\n",
      "2F4X_B\n",
      "2F88_A\n",
      "2BQ5_S\n",
      "2BS0_R\n",
      "2BQ5_R\n",
      "2BS0_S\n",
      "2G1W_A\n",
      "2B2E_R\n",
      "2B2E_S\n",
      "2GO5_9\n",
      "2GO5_A\n",
      "2GIP_A\n",
      "2GIO_A\n",
      "2FEY_A\n",
      "2AKE_B\n",
      "2DR2_B\n",
      "2AGN_C\n",
      "2AGN_A\n",
      "2IZN_S\n",
      "2IZ8_R\n",
      "2IZ8_S\n",
      "2HGH_B\n",
      "2F87_A\n",
      "2GV3_A\n",
      "2DER_D\n",
      "2DET_C\n",
      "2EVY_A\n",
      "2IXY_A\n",
      "2IXZ_A\n",
      "2HEM_A\n",
      "1ZBH_E\n",
      "2AHT_A\n",
      "2HNS_A\n",
      "2IY3_B\n",
      "2J28_8\n",
      "2J37_A\n",
      "2J28_A\n",
      "2NOQ_B\n",
      "2NOQ_E\n",
      "2NOQ_A\n",
      "2FDT_A\n",
      "2IHX_B\n",
      "2OB7_D\n",
      "2FY1_B\n",
      "2O33_A\n",
      "2IL9_M\n",
      "2IL9_A\n",
      "2GRW_A\n",
      "2DU5_D\n",
      "2DU4_C\n",
      "2DU6_D\n",
      "2GV4_A\n",
      "2HUA_A\n",
      "2GVO_A\n",
      "2PCW_A\n",
      "2NR0_H\n",
      "2NR0_G\n",
      "2NR0_F\n",
      "2NR0_E\n",
      "2OJ8_A\n",
      "2IZM_S\n",
      "2JR4_A\n",
      "2JPP_C\n",
      "2QH3_A\n",
      "2QH4_A\n",
      "2QH2_A\n",
      "2JTP_A\n",
      "2OM3_R\n",
      "2PN9_A\n",
      "2JSE_A\n",
      "2R93_R\n",
      "2JWV_A\n",
      "2OM7_F\n",
      "2OM7_J\n",
      "2OM7_C\n",
      "2OM7_I\n",
      "2OM7_G\n",
      "2OM7_A\n",
      "2OM7_H\n",
      "2RLU_A\n",
      "2R1G_C\n",
      "2R1G_A\n",
      "2R1G_X\n",
      "2R1G_F\n",
      "2R1G_B\n",
      "2R1G_E\n",
      "2JYM_A\n",
      "2JXV_A\n",
      "2ZJQ_Y\n",
      "2ZJQ_X\n",
      "3DEG_I\n",
      "3DEG_G\n",
      "3DKN_E\n",
      "3DEG_J\n",
      "3DKN_F\n",
      "3DEG_E\n",
      "3DKN_D\n",
      "3DEG_K\n",
      "2RN1_B\n",
      "2K95_A\n",
      "2K4C_A\n",
      "2W2H_S\n",
      "3EQ3_E\n",
      "3EQ3_Y\n",
      "3EP2_B\n",
      "3EP2_D\n",
      "3EQ4_A\n",
      "3EP2_C\n",
      "2RPK_A\n",
      "2RO2_A\n",
      "3CW1_v\n",
      "3HAY_E\n",
      "2K66_A\n",
      "2K5Z_A\n",
      "3A3A_A\n",
      "2KMJ_A\n",
      "2RPT_A\n",
      "3A2K_C\n",
      "2KOC_A\n",
      "2KD8_A\n",
      "2KE6_A\n",
      "2KHY_A\n",
      "2X7N_A\n",
      "2KUV_A\n",
      "2KUW_A\n",
      "2KUU_A\n",
      "2KUR_A\n",
      "2KPD_A\n",
      "2KPC_A\n",
      "2KVN_A\n",
      "3LWQ_D\n",
      "2KPV_A\n",
      "3JQ4_B\n",
      "3AKZ_H\n",
      "3IYR_A\n",
      "3IZ4_A\n",
      "2L3C_B\n",
      "2L1F_A\n",
      "2L2J_A\n",
      "2L1F_B\n",
      "2L3J_B\n",
      "2L3E_A\n",
      "3IZD_A\n",
      "2KRP_A\n",
      "3PGW_N\n",
      "2KRL_A\n",
      "2L5Z_A\n",
      "3NDB_M\n",
      "2KZL_A\n",
      "3PIP_Y\n",
      "2XXA_F\n",
      "2L6I_A\n",
      "3IZZ_E\n",
      "2LA5_A\n",
      "2LAC_A\n",
      "2LBS_A\n",
      "2LDT_A\n",
      "3AMU_B\n",
      "2Y95_A\n",
      "3TUP_T\n",
      "2LC8_A\n",
      "2LDL_A\n",
      "2LKR_A\n",
      "2LK3_A\n",
      "2LBJ_A\n",
      "2LBK_A\n",
      "2LBL_A\n",
      "2LPA_A\n",
      "2LP9_A\n",
      "4A4T_A\n",
      "4A4U_A\n",
      "4A4S_A\n",
      "3UZS_C\n",
      "2LHP_A\n",
      "2LUB_A\n",
      "2LQZ_A\n",
      "2LI4_A\n",
      "2LJJ_A\n",
      "4DR5_W\n",
      "4ILM_M\n",
      "2M21_A\n",
      "4ILL_C\n",
      "2M22_A\n",
      "2LU0_A\n",
      "3W1K_F\n",
      "3J3W_A\n",
      "3J3V_A\n",
      "3J3V_B\n",
      "2M8K_A\n",
      "2LV0_A\n",
      "4KJI_D\n",
      "4BY9_A\n",
      "2M12_A\n",
      "4C4Q_N\n",
      "2M23_A\n",
      "3WC2_Q\n",
      "2LUN_A\n",
      "3WC2_P\n",
      "3WC1_Q\n",
      "2MHI_A\n",
      "3WFR_D\n",
      "3WFQ_C\n",
      "2MEQ_A\n",
      "3WFR_B\n",
      "2MI0_B\n",
      "2MI0_A\n",
      "2MFG_D\n",
      "2MFC_B\n",
      "2M57_A\n",
      "2MFE_D\n",
      "2MFF_B\n",
      "2M4W_A\n",
      "2M5U_A\n",
      "3J6B_e\n",
      "4KR6_D\n",
      "2MF0_G\n",
      "4P8Z_A\n",
      "3WQZ_C\n",
      "4V6X_BC\n",
      "4V99_FX\n",
      "4V4N_A3\n",
      "4V6W_A7\n",
      "4V4N_B2\n",
      "4V5Z_BP\n",
      "4V4V_B0\n",
      "4V4W_B9\n",
      "4V8J_CW\n",
      "4V4V_AA\n",
      "4V6X_B2\n",
      "4V6W_A8\n",
      "4V4G_CA\n",
      "4V6U_A0\n",
      "4V6W_B2\n",
      "4V9B_CD\n",
      "4V4N_B1\n",
      "4V5G_BA\n",
      "4V7E_Ac\n",
      "4V65_A1\n",
      "4V6W_BC\n",
      "4V6X_A5\n",
      "4V5Z_BA\n",
      "4V5G_BB\n",
      "4V5Z_BH\n",
      "4V6D_AV\n",
      "4V7E_Ab\n",
      "4V5Z_BL\n",
      "4V6X_A7\n",
      "4V65_BB\n",
      "4V6X_A8\n",
      "4V5Z_BC\n",
      "4V8J_AV\n",
      "4V7E_Ad\n",
      "4V6U_B1\n",
      "4V7E_Aa\n",
      "4V5Z_BK\n",
      "4V5Z_AH\n",
      "4V4N_A1\n",
      "4V6W_A5\n",
      "4V5Z_BU\n",
      "4V7E_Ae\n",
      "4V5F_CA\n",
      "4V5Z_BQ\n",
      "4V5Z_AF\n",
      "4V5Z_BM\n",
      "4QIL_C\n",
      "4OQ9_3\n",
      "4TZV_B\n",
      "4TZP_C\n",
      "2MS1_B\n",
      "2MQT_A\n",
      "2MNC_A\n",
      "2MFD_A\n",
      "2MTJ_A\n",
      "4WSA_V\n",
      "3J8G_B\n",
      "3J8G_A\n",
      "4WJ3_Q\n",
      "4WZJ_VV\n",
      "4D5N_X\n",
      "4X4P_B\n",
      "4X4S_B\n",
      "4X0B_B\n",
      "4X0A_B\n",
      "4WC3_B\n",
      "5A18_A\n",
      "2MXJ_A\n",
      "2MXL_A\n",
      "4TUC_QY\n",
      "4TUA_XY\n",
      "2N0R_A\n",
      "2N1Q_A\n",
      "5AMQ_C\n",
      "4ZT9_D\n",
      "2N3Q_A\n",
      "4YVI_C\n",
      "4YVJ_C\n",
      "4YVK_C\n",
      "2N2P_A\n",
      "2N2O_A\n",
      "3JB9_C\n",
      "2N8V_X\n",
      "2N4L_A\n",
      "2N7M_X\n",
      "5ELS_I\n",
      "5FMZ_V\n",
      "5EPI_H\n",
      "5GAP_U\n",
      "5GAP_V\n",
      "2N7X_A\n",
      "4Z7L_F\n",
      "5FJ1_H\n",
      "3JD5_A\n",
      "2N6X_A\n",
      "2N6T_A\n",
      "2N6W_A\n",
      "2N6S_A\n",
      "2N3O_B\n",
      "2NBY_A\n",
      "2NC1_A\n",
      "2NC0_A\n",
      "5KK5_B\n",
      "2NBZ_A\n",
      "2NBX_A\n",
      "5KQE_A\n",
      "2NCI_A\n",
      "5KMZ_A\n",
      "5IEM_A\n",
      "5MC6_BR\n",
      "5MC6_BS\n",
      "5MC6_m\n",
      "5MC6_n\n",
      "2RVO_A\n",
      "5MS0_R\n",
      "5UZT_A\n",
      "5UF3_A\n",
      "5XBL_B\n",
      "5WQ1_A\n",
      "5V16_A\n",
      "5V17_A\n",
      "5KH8_A\n",
      "5OA3_1\n",
      "5O7H_A\n",
      "5LSN_A\n",
      "5V93_B\n",
      "5V93_a\n",
      "5V6X_C\n",
      "5V6X_D\n",
      "6AWD_A\n",
      "6AWC_A\n",
      "5XJ2_G\n",
      "5Y36_B\n",
      "6EVJ_V\n",
      "6C4H_x\n",
      "5N5C_A\n",
      "6FLQ_R\n",
      "6FT6_2\n",
      "5Z3G_B\n",
      "5ZAL_C\n",
      "5ZAM_C\n",
      "6C66_J\n",
      "5ZEY_B\n",
      "6DU5_B\n",
      "6AH3_T\n",
      "6AGB_A\n",
      "6MCE_A\n",
      "6BZ7_QW\n",
      "6BZ8_XY\n",
      "6BZ7_XY\n",
      "6AHU_A\n",
      "6AHU_T\n",
      "6CYT_N\n",
      "6MJ0_B\n",
      "6IFO_D\n",
      "6FHI_V\n",
      "6GBM_A\n",
      "6NDK_XY\n",
      "6AEB_E\n",
      "6IV6_G\n",
      "6QX9_1\n",
      "6QW6_5\n",
      "6HYK_A\n",
      "6DTI_X\n",
      "6AAX_D\n",
      "6NM9_G\n",
      "6K0B_U\n",
      "6K0A_X\n",
      "6O0Z_B\n",
      "6O0Y_B\n",
      "6S0X_B\n",
      "6S0Z_B\n",
      "6S0X_a\n",
      "6HYU_D\n",
      "6QWL_W\n",
      "6RR7_D\n",
      "6QX3_D\n",
      "6N7R_R\n",
      "6K4S_A\n",
      "6K3Z_A\n",
      "6KUT_V\n",
      "6KUV_V\n",
      "6POM_B\n",
      "6UFM_A\n",
      "6RFL_U\n",
      "6UES_A\n",
      "6SDW_B\n",
      "6SY6_D\n",
      "6V5B_D\n",
      "6MXQ_A\n",
      "6WPI_C\n",
      "6XWW_A\n",
      "6XWJ_A\n",
      "6ORD_QY\n",
      "6WB1_C\n",
      "6OJ2_QW\n",
      "6WB0_C\n",
      "6OF6_XW\n",
      "6OPE_XY\n",
      "6OJ2_QY\n",
      "6PK9_A\n",
      "6WLO_A\n",
      "6WLT_A\n",
      "6WD2_1\n",
      "6WD9_2\n",
      "6WLN_A\n",
      "6WLS_A\n",
      "6WLM_A\n",
      "6WLK_A\n",
      "6NOA_A\n",
      "6Z6B_UUU\n",
      "6XRZ_A\n",
      "6U79_A\n",
      "6VZC_A\n",
      "6W62_B\n",
      "6W64_B\n",
      "6W5C_B\n",
      "6W3M_A\n",
      "6O3M_XV\n",
      "6OSI_RB\n",
      "7D1A_A\n",
      "6LTP_H\n",
      "6XZR_IN1\n",
      "6Y0C_IN1\n",
      "6XZD_IN1\n",
      "6ZVK_d2\n",
      "6ZVK_h2\n",
      "6ZVK_K3\n",
      "6ZVK_E1\n",
      "6ZVK_e2\n",
      "7A01_e2\n",
      "7A01_K3\n",
      "6VAR_A\n",
      "7B9V_5\n",
      "7B9V_2\n",
      "7LVA_A\n",
      "7LJY_B\n",
      "6ZDU_C\n",
      "7LMA_B\n",
      "7LYG_A\n",
      "7LMB_B\n",
      "7D8C_B\n",
      "7DD4_A\n",
      "7OBQ_1\n",
      "7M5O_B\n",
      "7LYT_B\n",
      "7LYS_B\n",
      "7NQ4_C\n",
      "7ELE_G\n",
      "7JRT_B\n",
      "7JRS_B\n",
      "7K4L_A\n",
      "7DLZ_Y\n",
      "6WW6_E\n",
      "6WW6_F\n",
      "6WW6_C\n",
      "7NK4_D\n",
      "7SAM_A\n",
      "7M57_cc\n",
      "7M57_ii\n",
      "7M2T_ss\n",
      "7RQ5_A\n",
      "7LHD_A\n",
      "7RYG_B\n",
      "7ORJ_H\n",
      "7ORM_H\n",
      "7ORI_H\n",
      "7EXY_A\n",
      "7S4V_B\n",
      "7S4U_B\n",
      "7WB1_D\n",
      "7SLP_R\n",
      "7UGA_A\n",
      "7S36_R\n",
      "7S38_R\n",
      "7S3H_R\n",
      "7KUB_A\n",
      "7KUD_A\n",
      "7KUC_A\n",
      "7X34_3\n",
      "7V06_A\n",
      "7UMC_A\n",
      "7UMD_A\n",
      "7UME_A\n",
      "7XSN_N\n",
      "7URM_v\n",
      "7UR5_y\n",
      "7V59_C\n",
      "7UVT_A\n",
      "7VTN_B\n",
      "7U2A_C\n",
      "7UO0_C\n",
      "7UO1_C\n",
      "7PTL_B\n",
      "7PTQ_C\n",
      "7PTS_A\n",
      "7OZS_3\n",
      "7SHX_A\n",
      "7QGG_v\n",
      "7RGU_H\n",
      "8AW3_1\n",
      "8DVS_B\n",
      "8DK7_C\n",
      "8CTI_C\n",
      "7QDU_Q\n",
      "8DC2_B\n",
      "8BTZ_A\n",
      "8BD5_B\n",
      "8D4A_B\n",
      "8D49_B\n",
      "8EXY_R\n",
      "7YOJ_B\n",
      "7ZPQ_6\n",
      "8FCS_A\n",
      "7ZRS_6\n",
      "8E29_B\n",
      "7XW2_C\n",
      "7VW3_B\n",
      "8E79_R\n",
      "7ZAP_B\n",
      "8C98_A\n",
      "8C8X_B\n",
      "8C9C_A\n",
      "7YR7_A\n",
      "8G1I_B\n",
      "7ZRZ_ZN1\n",
      "8FZT_B\n",
      "8G1S_R\n",
      "8H69_5\n",
      "8GAP_B\n",
      "8SCF_A\n",
      "8SCH_A\n",
      "8CI5_C\n",
      "8CLR_A\n",
      "8SA6_A\n",
      "8IDF_B\n",
      "8EWG_B\n",
      "8BWT_A\n",
      "8EVQ_EC\n",
      "8EVR_EC\n",
      "8CQ1_A\n",
      "8U3M_A\n",
      "8UYL_A\n",
      "8UYS_A\n",
      "8UYJ_A\n",
      "8UYP_A\n",
      "8OMR_C\n",
      "8HUD_B\n",
      "8T2B_R\n",
      "8UPT_A\n",
      "8T2O_R\n",
      "8T29_R\n",
      "8T2A_R\n",
      "8VCI_A\n",
      "8BU8_A\n",
      "8T2P_B\n",
      "8T2P_A\n",
      "8PM4_B\n",
      "8IEW_B\n",
      "8TOC_R\n",
      "8J72_C\n",
      "8QO3_A\n",
      "8QO4_A\n",
      "8QO5_A\n",
      "8QO2_A\n",
      "8TVZ_C\n",
      "8H6E_4A\n",
      "8H6E_5A\n",
      "8P0B_V\n",
      "8P0G_V\n",
      "8JHP_A\n",
      "8Z85_D\n",
      "8Z90_D\n",
      "8Z8N_D\n",
      "8KAI_I\n",
      "8KAM_A\n",
      "8K8B_A\n",
      "8JTJ_B\n",
      "8WUS_B\n",
      "8YGJ_B\n",
      "8UO6_B\n",
      "8JTR_B\n",
      "8KAG_A\n",
      "8URY_d\n",
      "8URY_A\n",
      "9EOW_A\n",
      "8WT6_E\n",
      "8WT8_F\n",
      "8WT8_E\n",
      "8WT6_F\n",
      "8WT7_E\n",
      "8WT7_F\n",
      "8SFO_B\n",
      "8SFI_B\n",
      "8SFL_B\n",
      "8SFN_B\n",
      "8SFH_B\n",
      "8SFJ_B\n",
      "9ENF_C\n",
      "9ENE_D\n",
      "9ENC_D\n",
      "8OKD_X\n",
      "8OST_C\n",
      "8OPT_D\n",
      "8OPP_D\n",
      "8OPS_C\n",
      "8I0P_F\n",
      "8KDA_O\n",
      "8UAU_R\n",
      "9CF1_W\n",
      "9CF3_W\n",
      "8YE6_C\n",
      "8KI7_E\n",
      "9G8O_S2\n",
      "8UBE_I\n",
      "9DTT_B\n",
      "9G7C_A\n",
      "9FO8_A\n",
      "9FO9_A\n",
      "8XPP_B\n",
      "8T2Y_tA\n",
      "8T2Y_tP\n",
      "8XCA_B\n",
      "8T3E_EC\n",
      "8T3F_EC\n",
      "8XCC_B\n",
      "8Z1G_T\n",
      "8Z1F_T\n",
      "R1107\n",
      "R1108\n",
      "R1116\n",
      "R1117v2\n",
      "R1126\n",
      "R1128\n",
      "R1136\n",
      "R1138\n",
      "R1149\n",
      "R1156\n",
      "R1189\n",
      "R1190\n",
      "R1107\n",
      "R1108\n",
      "R1116\n",
      "R1117v2\n",
      "R1126\n",
      "R1128\n",
      "R1136\n",
      "R1138\n",
      "R1149\n",
      "R1156\n",
      "R1189\n",
      "R1190\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"/srv/mingyang/dev/personal/rna-fold/data/\"\n",
    "train_csv = data_folder + \"train_sequences.csv\"\n",
    "val_csv = data_folder + \"validation_sequences.csv\"\n",
    "test_csv = data_folder + \"test_sequences.csv\"\n",
    "import pandas as pd\n",
    "id_to_seq = dict()\n",
    "def prepare_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for idx, row in df.iterrows():\n",
    "        seq_id = row[\"target_id\"]\n",
    "        print(seq_id)\n",
    "        seq = row['sequence']\n",
    "        # create a file {data_folder}/MSA/{seq_id}.fasta, write the sequence to the file\n",
    "        with open(f\"{data_folder}/MSA/{seq_id}.fasta\", \"w\") as f:\n",
    "            f.write(f\">{seq_id}\\n{seq}\")\n",
    "        id_to_seq[seq_id] = seq\n",
    "\n",
    "def split_to_csv(csv_path, split=\"train\"): # train, val, test\n",
    "    if split == \"train\":\n",
    "        csv_path = train_csv\n",
    "    elif split == \"val\":\n",
    "        csv_path = val_csv\n",
    "    elif split == \"test\":\n",
    "        csv_path = test_csv\n",
    "    return csv_path\n",
    "\n",
    "def all_seq_ids(split=\"train\"): # train, val, test\n",
    "    csv_path = split_to_csv(split)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[\"target_id\"].tolist()\n",
    "\n",
    "prepare_csv(train_csv)\n",
    "prepare_csv(val_csv)\n",
    "prepare_csv(test_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1SCL_A', '1RNK_A', '1RHT_A', '1HLX_A', '1HMH_E', '1RNG_A', '1MME_D', '1KAJ_A', '1SLO_A', '1BIV_A', '1ANR_A', '1ZIG_A', '1ZIH_A', '1ETF_A', '1ZIF_A', '1KPD_A', '1IKD_A', '1ZDI_S', '1AFX_A', '1EBQ_A', '1EBR_A', '1ULL_A', '1KIS_B', '1KIS_A', '1ATO_A', '1TLR_A', '1VOP_A', '1AQO_A', '1ATV_A', '1ATW_A', '1UUU_A', '1AUD_B', '2U2A_A', '1A4T_A', '1A60_A', '1A51_A', '2A9L_A', '1A1T_B', '1A9N_Q', '3PHP_A', '2TPK_A', '7MSF_S', '5MSF_S', '1LDZ_A', '1ZDK_S', '1BVJ_A', '1B36_A', '1HVU_I', '1BGZ_A', '2BJ2_B', '2BJ2_A', '28SP_A', '1QFQ_A', '17RA_A', '1BAU_B', '1BZ2_A', '1CQ5_A', '1QC8_A', '484D_B', '1D6K_B', '1EIY_C', '1ESH_A', '1EXY_A', '1D0U_A', '1ESY_A', '1EUQ_B', '1F9L_A', '1FFK_9', '1EKZ_B', '1F5U_B', '1F6Z_A', '1F6X_A', '1F85_A', '1F84_A', '1FOQ_A', '1FJE_A', '1E4P_A', '1FQZ_A', '1G70_A', '1E7K_D', '1FYO_A', '1FHK_A', '1I3X_A', '1I4C_A', '1I46_A', '1IBM_Y', '1HWQ_A', '1IK1_A', '1E95_A', '1HS2_A', '1JO7_A', '1IDV_A', '1K5I_A', '1K9W_A', '1JTW_A', '1K6G_A', '1K4B_A', '1K4A_A', '1K6H_A', '1JUR_A', '1KKS_A', '1JWC_A', '1L1C_C', '1KP7_A', '1JOX_A', '1K2G_A', '1L1W_A', '1LC6_A', '1LS2_B', '1KKA_A', '1MFY_A', '1M5L_A', '1MFJ_A', '1MFK_A', '1N34_A', '1JTJ_A', '1MT4_A', '1MNX_A', '1NA2_A', '1NC0_A', '1N8X_A', '1OQ0_A', '1OW9_A', '1OSW_A', '1M82_A', '1NYB_B', '1PJY_A', '1P6V_B', '1P6V_D', '1N66_A', '1HS1_A', '1HS8_A', '1HS4_A', '1HS3_A', '1JZC_A', '1QZC_C', '1P5M_A', '1R2W_C', '1P5N_A', '1QZC_B', '1P5P_A', '1QZA_B', '1QZB_B', '1QWB_A', '1QWA_A', '1Q75_A', '1R2P_A', '1RFR_A', '1S9S_A', '1RY1_E', '1SZY_A', '1ROQ_A', '1R7W_A', '1R7Z_A', '1T4L_A', '1WKS_A', '1T28_A', '1S34_A', '1TJZ_A', '1TXS_A', '1R4H_A', '1XHP_A', '1XSG_A', '1Y1Y_P', '1XSH_A', '1YLG_A', '1YNE_A', '1XWU_A', '1XWP_A', '1TBK_A', '1YMO_A', '1Z31_A', '1U63_D', '1ZC8_Z', '1ZC8_G', '1ZC8_J', '1ZC8_F', '1ZC8_I', '1ZC8_H', '1Z30_A', '1X18_D', '1X18_A', '1X18_B', '1ZC5_A', '1ZN1_B', '1Z2J_A', '1YSH_B', '1YSH_A', '1YSH_F', '1YSV_A', '2ADT_A', '1WZ2_D', '2A64_A', '2D19_A', '2D1B_B', '2EUY_A', '2B6G_B', '2ESE_B', '2F4X_B', '2F88_A', '2BQ5_S', '2BS0_R', '2BQ5_R', '2BS0_S', '2G1W_A', '2B2E_R', '2B2E_S', '2GO5_9', '2GO5_A', '2GIP_A', '2GIO_A', '2FEY_A', '2AKE_B', '2DR2_B', '2AGN_C', '2AGN_A', '2IZN_S', '2IZ8_R', '2IZ8_S', '2HGH_B', '2F87_A', '2GV3_A', '2DER_D', '2DET_C', '2EVY_A', '2IXY_A', '2IXZ_A', '2HEM_A', '1ZBH_E', '2AHT_A', '2HNS_A', '2IY3_B', '2J28_8', '2J37_A', '2J28_A', '2NOQ_B', '2NOQ_E', '2NOQ_A', '2FDT_A', '2IHX_B', '2OB7_D', '2FY1_B', '2O33_A', '2IL9_M', '2IL9_A', '2GRW_A', '2DU5_D', '2DU4_C', '2DU6_D', '2GV4_A', '2HUA_A', '2GVO_A', '2PCW_A', '2NR0_H', '2NR0_G', '2NR0_F', '2NR0_E', '2OJ8_A', '2IZM_S', '2JR4_A', '2JPP_C', '2QH3_A', '2QH4_A', '2QH2_A', '2JTP_A', '2OM3_R', '2PN9_A', '2JSE_A', '2R93_R', '2JWV_A', '2OM7_F', '2OM7_J', '2OM7_C', '2OM7_I', '2OM7_G', '2OM7_A', '2OM7_H', '2RLU_A', '2R1G_C', '2R1G_A', '2R1G_X', '2R1G_F', '2R1G_B', '2R1G_E', '2JYM_A', '2JXV_A', '2ZJQ_Y', '2ZJQ_X', '3DEG_I', '3DEG_G', '3DKN_E', '3DEG_J', '3DKN_F', '3DEG_E', '3DKN_D', '3DEG_K', '2RN1_B', '2K95_A', '2K4C_A', '2W2H_S', '3EQ3_E', '3EQ3_Y', '3EP2_B', '3EP2_D', '3EQ4_A', '3EP2_C', '2RPK_A', '2RO2_A', '3CW1_v', '3HAY_E', '2K66_A', '2K5Z_A', '3A3A_A', '2KMJ_A', '2RPT_A', '3A2K_C', '2KOC_A', '2KD8_A', '2KE6_A', '2KHY_A', '2X7N_A', '2KUV_A', '2KUW_A', '2KUU_A', '2KUR_A', '2KPD_A', '2KPC_A', '2KVN_A', '3LWQ_D', '2KPV_A', '3JQ4_B', '3AKZ_H', '3IYR_A', '3IZ4_A', '2L3C_B', '2L1F_A', '2L2J_A', '2L1F_B', '2L3J_B', '2L3E_A', '3IZD_A', '2KRP_A', '3PGW_N', '2KRL_A', '2L5Z_A', '3NDB_M', '2KZL_A', '3PIP_Y', '2XXA_F', '2L6I_A', '3IZZ_E', '2LA5_A', '2LAC_A', '2LBS_A', '2LDT_A', '3AMU_B', '2Y95_A', '3TUP_T', '2LC8_A', '2LDL_A', '2LKR_A', '2LK3_A', '2LBJ_A', '2LBK_A', '2LBL_A', '2LPA_A', '2LP9_A', '4A4T_A', '4A4U_A', '4A4S_A', '3UZS_C', '2LHP_A', '2LUB_A', '2LQZ_A', '2LI4_A', '2LJJ_A', '4DR5_W', '4ILM_M', '2M21_A', '4ILL_C', '2M22_A', '2LU0_A', '3W1K_F', '3J3W_A', '3J3V_A', '3J3V_B', '2M8K_A', '2LV0_A', '4KJI_D', '4BY9_A', '2M12_A', '4C4Q_N', '2M23_A', '3WC2_Q', '2LUN_A', '3WC2_P', '3WC1_Q', '2MHI_A', '3WFR_D', '3WFQ_C', '2MEQ_A', '3WFR_B', '2MI0_B', '2MI0_A', '2MFG_D', '2MFC_B', '2M57_A', '2MFE_D', '2MFF_B', '2M4W_A', '2M5U_A', '3J6B_e', '4KR6_D', '2MF0_G', '4P8Z_A', '3WQZ_C', '4V6X_BC', '4V99_FX', '4V4N_A3', '4V6W_A7', '4V4N_B2', '4V5Z_BP', '4V4V_B0', '4V4W_B9', '4V8J_CW', '4V4V_AA', '4V6X_B2', '4V6W_A8', '4V4G_CA', '4V6U_A0', '4V6W_B2', '4V9B_CD', '4V4N_B1', '4V5G_BA', '4V7E_Ac', '4V65_A1', '4V6W_BC', '4V6X_A5', '4V5Z_BA', '4V5G_BB', '4V5Z_BH', '4V6D_AV', '4V7E_Ab', '4V5Z_BL', '4V6X_A7', '4V65_BB', '4V6X_A8', '4V5Z_BC', '4V8J_AV', '4V7E_Ad', '4V6U_B1', '4V7E_Aa', '4V5Z_BK', '4V5Z_AH', '4V4N_A1', '4V6W_A5', '4V5Z_BU', '4V7E_Ae', '4V5F_CA', '4V5Z_BQ', '4V5Z_AF', '4V5Z_BM', '4QIL_C', '4OQ9_3', '4TZV_B', '4TZP_C', '2MS1_B', '2MQT_A', '2MNC_A', '2MFD_A', '2MTJ_A', '4WSA_V', '3J8G_B', '3J8G_A', '4WJ3_Q', '4WZJ_VV', '4D5N_X', '4X4P_B', '4X4S_B', '4X0B_B', '4X0A_B', '4WC3_B', '5A18_A', '2MXJ_A', '2MXL_A', '4TUC_QY', '4TUA_XY', '2N0R_A', '2N1Q_A', '5AMQ_C', '4ZT9_D', '2N3Q_A', '4YVI_C', '4YVJ_C', '4YVK_C', '2N2P_A', '2N2O_A', '3JB9_C', '2N8V_X', '2N4L_A', '2N7M_X', '5ELS_I', '5FMZ_V', '5EPI_H', '5GAP_U', '5GAP_V', '2N7X_A', '4Z7L_F', '5FJ1_H', '3JD5_A', '2N6X_A', '2N6T_A', '2N6W_A', '2N6S_A', '2N3O_B', '2NBY_A', '2NC1_A', '2NC0_A', '5KK5_B', '2NBZ_A', '2NBX_A', '5KQE_A', '2NCI_A', '5KMZ_A', '5IEM_A', '5MC6_BR', '5MC6_BS', '5MC6_m', '5MC6_n', '2RVO_A', '5MS0_R', '5UZT_A', '5UF3_A', '5XBL_B', '5WQ1_A', '5V16_A', '5V17_A', '5KH8_A', '5OA3_1', '5O7H_A', '5LSN_A', '5V93_B', '5V93_a', '5V6X_C', '5V6X_D', '6AWD_A', '6AWC_A', '5XJ2_G', '5Y36_B', '6EVJ_V', '6C4H_x', '5N5C_A', '6FLQ_R', '6FT6_2', '5Z3G_B', '5ZAL_C', '5ZAM_C', '6C66_J', '5ZEY_B', '6DU5_B', '6AH3_T', '6AGB_A', '6MCE_A', '6BZ7_QW', '6BZ8_XY', '6BZ7_XY', '6AHU_A', '6AHU_T', '6CYT_N', '6MJ0_B', '6IFO_D', '6FHI_V', '6GBM_A', '6NDK_XY', '6AEB_E', '6IV6_G', '6QX9_1', '6QW6_5', '6HYK_A', '6DTI_X', '6AAX_D', '6NM9_G', '6K0B_U', '6K0A_X', '6O0Z_B', '6O0Y_B', '6S0X_B', '6S0Z_B', '6S0X_a', '6HYU_D', '6QWL_W', '6RR7_D', '6QX3_D', '6N7R_R', '6K4S_A', '6K3Z_A', '6KUT_V', '6KUV_V', '6POM_B', '6UFM_A', '6RFL_U', '6UES_A', '6SDW_B', '6SY6_D', '6V5B_D', '6MXQ_A', '6WPI_C', '6XWW_A', '6XWJ_A', '6ORD_QY', '6WB1_C', '6OJ2_QW', '6WB0_C', '6OF6_XW', '6OPE_XY', '6OJ2_QY', '6PK9_A', '6WLO_A', '6WLT_A', '6WD2_1', '6WD9_2', '6WLN_A', '6WLS_A', '6WLM_A', '6WLK_A', '6NOA_A', '6Z6B_UUU', '6XRZ_A', '6U79_A', '6VZC_A', '6W62_B', '6W64_B', '6W5C_B', '6W3M_A', '6O3M_XV', '6OSI_RB', '7D1A_A', '6LTP_H', '6XZR_IN1', '6Y0C_IN1', '6XZD_IN1', '6ZVK_d2', '6ZVK_h2', '6ZVK_K3', '6ZVK_E1', '6ZVK_e2', '7A01_e2', '7A01_K3', '6VAR_A', '7B9V_5', '7B9V_2', '7LVA_A', '7LJY_B', '6ZDU_C', '7LMA_B', '7LYG_A', '7LMB_B', '7D8C_B', '7DD4_A', '7OBQ_1', '7M5O_B', '7LYT_B', '7LYS_B', '7NQ4_C', '7ELE_G', '7JRT_B', '7JRS_B', '7K4L_A', '7DLZ_Y', '6WW6_E', '6WW6_F', '6WW6_C', '7NK4_D', '7SAM_A', '7M57_cc', '7M57_ii', '7M2T_ss', '7RQ5_A', '7LHD_A', '7RYG_B', '7ORJ_H', '7ORM_H', '7ORI_H', '7EXY_A', '7S4V_B', '7S4U_B', '7WB1_D', '7SLP_R', '7UGA_A', '7S36_R', '7S38_R', '7S3H_R', '7KUB_A', '7KUD_A', '7KUC_A', '7X34_3', '7V06_A', '7UMC_A', '7UMD_A', '7UME_A', '7XSN_N', '7URM_v', '7UR5_y', '7V59_C', '7UVT_A', '7VTN_B', '7U2A_C', '7UO0_C', '7UO1_C', '7PTL_B', '7PTQ_C', '7PTS_A', '7OZS_3', '7SHX_A', '7QGG_v', '7RGU_H', '8AW3_1', '8DVS_B', '8DK7_C', '8CTI_C', '7QDU_Q', '8DC2_B', '8BTZ_A', '8BD5_B', '8D4A_B', '8D49_B', '8EXY_R', '7YOJ_B', '7ZPQ_6', '8FCS_A', '7ZRS_6', '8E29_B', '7XW2_C', '7VW3_B', '8E79_R', '7ZAP_B', '8C98_A', '8C8X_B', '8C9C_A', '7YR7_A', '8G1I_B', '7ZRZ_ZN1', '8FZT_B', '8G1S_R', '8H69_5', '8GAP_B', '8SCF_A', '8SCH_A', '8CI5_C', '8CLR_A', '8SA6_A', '8IDF_B', '8EWG_B', '8BWT_A', '8EVQ_EC', '8EVR_EC', '8CQ1_A', '8U3M_A', '8UYL_A', '8UYS_A', '8UYJ_A', '8UYP_A', '8OMR_C', '8HUD_B', '8T2B_R', '8UPT_A', '8T2O_R', '8T29_R', '8T2A_R', '8VCI_A', '8BU8_A', '8T2P_B', '8T2P_A', '8PM4_B', '8IEW_B', '8TOC_R', '8J72_C', '8QO3_A', '8QO4_A', '8QO5_A', '8QO2_A', '8TVZ_C', '8H6E_4A', '8H6E_5A', '8P0B_V', '8P0G_V', '8JHP_A', '8Z85_D', '8Z90_D', '8Z8N_D', '8KAI_I', '8KAM_A', '8K8B_A', '8JTJ_B', '8WUS_B', '8YGJ_B', '8UO6_B', '8JTR_B', '8KAG_A', '8URY_d', '8URY_A', '9EOW_A', '8WT6_E', '8WT8_F', '8WT8_E', '8WT6_F', '8WT7_E', '8WT7_F', '8SFO_B', '8SFI_B', '8SFL_B', '8SFN_B', '8SFH_B', '8SFJ_B', '9ENF_C', '9ENE_D', '9ENC_D', '8OKD_X', '8OST_C', '8OPT_D', '8OPP_D', '8OPS_C', '8I0P_F', '8KDA_O', '8UAU_R', '9CF1_W', '9CF3_W', '8YE6_C', '8KI7_E', '9G8O_S2', '8UBE_I', '9DTT_B', '9G7C_A', '9FO8_A', '9FO9_A', '8XPP_B', '8T2Y_tA', '8T2Y_tP', '8XCA_B', '8T3E_EC', '8T3F_EC', '8XCC_B', '8Z1G_T', '8Z1F_T']\n",
      "GGGUGCUCAGUACGAGAGGAACCGCACCC\n",
      "torch.Size([1, 128, 29])\n",
      "torch.Size([1, 29])\n",
      "10\n",
      "dict_keys(['frames', 'unnormalized_angles', 'angles', 'single', 'cord_tns_pred', \"cords_c1'\", 'plddt', 'ss', 'p', 'c4_', 'n'])\n"
     ]
    }
   ],
   "source": [
    "# read the csv file\n",
    "import pandas as pd\n",
    "\n",
    "from rhofold.utils.alphabet import get_features\n",
    "\n",
    "def seq_id_to_data(seq_id):\n",
    "    with torch.inference_mode():\n",
    "        fasta_path = f\"{data_folder}/MSA/{seq_id}.fasta\"  # This should contain just your query sequence\n",
    "        msa_path = f\"{data_folder}/MSA/{seq_id}.MSA.fasta\"  # This is your MSA file\n",
    "\n",
    "        # Get features (optionally specify msa_depth, default is 128)\n",
    "        features = get_features(\n",
    "            fas_fpath=fasta_path,\n",
    "            msa_fpath=msa_path,\n",
    "            msa_depth=128,  # You can adjust this if you want to use more/fewer sequences from the MSA\n",
    "        )\n",
    "        print(features[\"seq\"])\n",
    "        print(features[\"tokens\"].shape)\n",
    "        print(features[\"rna_fm_tokens\"].shape)\n",
    "\n",
    "        device = \"cuda\"\n",
    "        outputs = model(\n",
    "            tokens=features[\"tokens\"].to(device),\n",
    "            rna_fm_tokens=features[\"rna_fm_tokens\"].to(device),\n",
    "            seq=features[\"seq\"],\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "train_list = all_seq_ids(train_csv)\n",
    "print(train_list)\n",
    "seq_id = train_list[0]\n",
    "outputs = seq_id_to_data(seq_id)\n",
    "print(len(outputs))\n",
    "print(outputs[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kabsch_align(P, Q):\n",
    "    # P, Q: [L, 3]\n",
    "    P_mean = P.mean(dim=0)\n",
    "    Q_mean = Q.mean(dim=0)\n",
    "    P_centered = P - P_mean\n",
    "    Q_centered = Q - Q_mean\n",
    "\n",
    "    H = P_centered.T @ Q_centered\n",
    "    U, S, Vt = torch.svd(H)\n",
    "    R = Vt @ U.T\n",
    "    if torch.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt @ U.T\n",
    "    P_aligned = P_centered @ R\n",
    "    return P_aligned, Q_centered\n",
    "\n",
    "\n",
    "def tm_score(P, Q):\n",
    "    # P: predicted coords, Q: true coords, both [L, 3]\n",
    "    L = P.shape[0]\n",
    "    d0 = 0.0\n",
    "    if L < 12:\n",
    "        d0 = 0.3\n",
    "    elif L < 16:\n",
    "        d0 = 0.4\n",
    "    elif L < 20:\n",
    "        d0 = 0.5\n",
    "    elif L < 24:\n",
    "        d0 = 0.6\n",
    "    elif L < 30:\n",
    "        d0 = 0.7\n",
    "    else:\n",
    "        d0 = 0.6 * (L - 0.5) ** 0.5 - 2.5\n",
    "\n",
    "    P_aligned, Q_centered = kabsch_align(P, Q)\n",
    "    dist = torch.norm(P_aligned - Q_centered, dim=1)\n",
    "    score = (1 / (1 + (dist / d0) ** 2)).mean()\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = \"/srv/mingyang/dev/personal/rna-fold/data/train_labels.csv\"\n",
    "val_labels = \"/srv/mingyang/dev/personal/rna-fold/data/validation_labels.csv\"\n",
    "id_to_loc = dict()\n",
    "for csv in [train_labels, val_labels]:\n",
    "    df = pd.read_csv(csv)\n",
    "    for idx, row in df.iterrows():\n",
    "        id = row[\"ID\"]\n",
    "        id = \"_\".join(id.split(\"_\")[:-1])\n",
    "        x_1, y_1, z_1 = row[\"x_1\"], row[\"y_1\"], row[\"z_1\"]\n",
    "        if id not in id_to_loc:\n",
    "            id_to_loc[id] = [(x_1, y_1, z_1)]\n",
    "        else:\n",
    "            id_to_loc[id].append((x_1, y_1, z_1))\n",
    "\n",
    "import torch\n",
    "for idx in id_to_loc.keys():\n",
    "    id_to_loc[idx] = torch.tensor(id_to_loc[idx], dtype=torch.float32, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGGAUAACUUCGGUUGUCCC\n",
      "torch.Size([1, 11, 20])\n",
      "torch.Size([1, 20])\n",
      "10\n",
      "dict_keys(['frames', 'unnormalized_angles', 'angles', 'single', 'cord_tns_pred', \"cords_c1'\", 'plddt', 'ss', 'p', 'c4_', 'n'])\n",
      "0.0027740008663386106\n",
      "0.003528591711074114\n",
      "0.009859115816652775\n",
      "0.009748130105435848\n",
      "0.00966308917850256\n",
      "0.009605878964066505\n",
      "0.009564719162881374\n",
      "0.009551241993904114\n",
      "0.009551722556352615\n",
      "0.009569044224917889\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    seq_id = train_list[3]\n",
    "    outputs = seq_id_to_data(seq_id)\n",
    "    print(len(outputs))\n",
    "    print(outputs[0].keys())\n",
    "    for otp in outputs:\n",
    "        preds = otp[\"cords_c1'\"][0][0]\n",
    "        score = tm_score(preds, id_to_loc[seq_id])\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = all_seq_ids(test_csv)\n",
    "print(len(test_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
